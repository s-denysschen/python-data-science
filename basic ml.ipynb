{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic machine learning\n",
    "There are three main types of statistical learning methods: supervised learning, unsupervised learning, and reinforcement learning. Each method has its own problem space and appropriate use cases.\n",
    "\n",
    "### Supervised Learning: \n",
    "This is where you have input variables (X) and an output variable (Y), and you use an algorithm to learn the mapping function from the input to the output. The ultimate goal is to approximate the mapping function so well that when you have new input data (X), you can predict the output variables (Y) for that data.\n",
    "\n",
    "It is called \"supervised learning\" because the process of an algorithm learning from the training dataset is akin to a teacher supervising the learning process. We know the correct answers; the algorithm iteratively makes predictions on the training data and is corrected by the teacher.\n",
    "\n",
    "The two main types of supervised learning problems are:\n",
    "\n",
    "* Regression: The output variable is a real or continuous value, such as \"salary\" or \"weight\". Algorithms used for these types of problems include Linear Regression, Decision Trees, and Support Vector Regression.\n",
    "* Classification: The output variable is a category, such as \"red\" or \"blue\" or \"disease\" and \"no disease\". Algorithms used for these types of problems include Logistic Regression, Naive Bayes, and Random Forest.\n",
    "\n",
    "### Unsupervised Learning: \n",
    "This is where you only have input data (X) and no corresponding output variables. The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\n",
    "\n",
    "These are called unsupervised learning because there is no correct answers and there is no teacher. Algorithms are left to their own to discover interesting structures in the data.\n",
    "\n",
    "The main types of unsupervised learning problems are:\n",
    "\n",
    "* Clustering: The task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). Algorithms include K-Means, Hierarchical Clustering, and DBSCAN.\n",
    "* Association: The task of finding interesting relationships or associations among a set of items. This is often used for market basket analysis. Algorithms include Apriori and FP-Growth.\n",
    "\n",
    "### Reinforcement Learning: \n",
    "It is about interaction between a learning agent and the environment. The agent takes actions in the environment to reach a certain goal. The environment, in return, gives reward or penalty (reinforcement signal) to the agent. The agent's task is to learn to make optimal decisions.\n",
    "\n",
    "A typical example is learning to play a game like chess. The agent decides the next move, the environment changes (the opponent makes a move), and the agent receives a reward (winning or losing the game)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SciKit-learn\n",
    "Scikit-learn is an open-source machine learning library in Python. It features various machine learning algorithms, including those for classification, regression, and clustering. It also provides tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n",
    "\n",
    "##### Classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = reg.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering: \n",
    "Scikit-learn provides several clustering algorithms like K-Means, Hierarchical clustering, DBSCAN, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Initialize the model\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get cluster labels for each sample\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn also provides many utility functions for preprocessing data, tuning hyperparameters, evaluating models, etc. that make the whole process of building and evaluating machine learning models easier and more efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "Model evaluation is a critical part of the machine learning pipeline. Once we've trained our model, we need to know how well it's performing. Model evaluation is a bit different for classification and regression problems due to the different nature of their output.\n",
    "#### Classification Model Evaluation Metrics:\n",
    "\n",
    "##### Accuracy: \n",
    "It is the ratio of correctly predicted observations to the total observations. However, it's not a good choice with imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix: \n",
    "A table used to describe the performance of a classification model. It presents a clear picture of precision, recall, F1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Precision: \n",
    "It is the ratio of correctly predicted positive observations to the total predicted positives.\n",
    "\n",
    "##### Recall (Sensitivity): \n",
    "It is the ratio of correctly predicted positive observations to the all observations in actual class.\n",
    "\n",
    "##### F1 Score: \n",
    "It is the weighted average of Precision and Recall. It tries to find the balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC-AUC: \n",
    "ROC curve is a graph showing the performance of a classification model at all classification thresholds. AUC stands for \"Area under the ROC Curve\". An excellent model has AUC near to 1, whereas a poor model has AUC near to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc = roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Model Evaluation Metrics:\n",
    "\n",
    "##### Mean Absolute Error (MAE): \n",
    "It is the mean of the absolute value of the errors. It's the easiest to understand, because it's the average error.\n",
    "\n",
    "##### Mean Squared Error (MSE): \n",
    "It is the mean of the squared errors. It's more popular than MAE, because MSE \"punishes\" larger errors.\n",
    "\n",
    "##### Root Mean Squared Error (RMSE): \n",
    "It is the square root of the mean of the squared errors. It measures the standard deviation of the residuals.\n",
    "\n",
    "##### R-squared (Coefficient of determination): \n",
    "Represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. A higher value is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of metric depends on your business objective. Sometimes, we might prefer a model with a higher recall than a high precision, for example in cancer prediction, we want to capture as many positives as possible. In another case like email spam detection, we want to be as precise as possible to not put important emails in the spam folder.\n",
    "\n",
    "In the case of regression, lower values of MAE, MSE, or RMSE suggest a better fit to the data. A higher R-squared indicates a higher proportion of variance explained by the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
